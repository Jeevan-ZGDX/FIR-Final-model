# -*- coding: utf-8 -*-
"""FIR_Law_Section_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vCjcRVUvXZKiHMo6Glj7IX5fqOglQV2D

# FIR to Law Section Classifier (Prototype)
This Colab notebook demonstrates a basic ML pipeline to classify FIR-like text into relevant IPC sections using a fine-tuned BERT model.
"""

# Install dependencies
!pip install transformers datasets scikit-learn
import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split

# Function to predict law sections for new text
def predict_law_sections(text, tokenizer, model, mlb):
    # Tokenize the input text
    encoding = tokenizer(text, truncation=True, padding=True, return_tensors='pt')

    # Move the encoding to the same device as the model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    encoding = {key: val.to(device) for key, val in encoding.items()}
    model.to(device)

    # Make predictions
    with torch.no_grad():
        outputs = model(**encoding)
        logits = outputs.logits
        # Apply sigmoid to get probabilities for multi-label classification
        probabilities = torch.sigmoid(logits)
        # Get predicted labels based on a threshold (e.g., 0.5)
        predictions = (probabilities > 0.5).cpu().numpy()

    # Inverse transform the binarized labels to get the original law sections
    predicted_labels = mlb.inverse_transform(predictions)
    return predicted_labels

# Example usage with a new case
new_case_text = "Someone stole my bicycle from outside my house during the day."
predicted_sections = predict_law_sections(new_case_text, tokenizer, model, mlb)

print(f"The predicted law sections for the case '{new_case_text}' are: {predicted_sections}")

# Sample mock FIR data (replace with real data later)
data = pd.DataFrame({
    'text': [
        'The accused broke into the house at night and stole valuable items.',
        'A man was assaulted in public with a sharp object.',
        'Woman reported repeated threats and stalking by neighbor.'
    ],
    'labels': [
        ['457', '380'],
        ['324'],
        ['506', '354D']
    ]
})

mlb = MultiLabelBinarizer()
labels = mlb.fit_transform(data['labels'])
train_texts, val_texts, train_labels, val_labels = train_test_split(data['text'], labels, test_size=0.2)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)
val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)

class FIRDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __len__(self):
        return len(self.labels)
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx]).float()
        return item

train_dataset = FIRDataset(train_encodings, train_labels)
val_dataset = FIRDataset(val_encodings, val_labels)

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(mlb.classes_), problem_type='multi_label_classification')

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    eval_strategy="epoch",  # Corrected parameter name
    save_strategy="no",
    logging_dir='./logs',
    report_to="none" # Disable W&B logging
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Start training (demo only)
trainer.train()

"""### Predicted Law Sections"""

from IPython.display import display, HTML

# You would need to create a dictionary or lookup function for actual law descriptions
law_descriptions = {
    '457': 'Lurking house-trespass or house-breaking by night in order to commit offence punishable with imprisonment.',
    '380': 'Theft in dwelling house, etc.',
    '324': 'Voluntarily causing hurt by dangerous weapons or means.',
    '506': 'Punishment for criminal intimidation.',
    '354D': 'Stalking.'
    # Add descriptions for other relevant sections here
}

def display_law_sections_with_radio_buttons(sections, case_text, descriptions):
    html_content = f"<b>Predicted Law Sections for Case: '{case_text}'</b><br>"
    for section_tuple in sections:
        for section in section_tuple: # Iterate through each section in the tuple
            description = descriptions.get(section, "Description not available") # Get description or use placeholder
            html_content += f'<input type="radio" id="{section}" name="law_section" value="{section}'>
            html_content += f'<label for="{section}">{section} - {description}</label><br>'
    display(HTML(html_content))

# Assuming 'predicted_sections' is the output from the predict_law_sections function
# and 'new_case_text' is the variable holding the case description
display_law_sections_with_radio_buttons(predicted_sections, new_case_text, law_descriptions)


